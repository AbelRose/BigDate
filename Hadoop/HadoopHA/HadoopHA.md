# HadoopHA
HadoopHA(Hadoop High Available) 高可用 必须有容错机制

HDFS ---> NN
YARN ---> RM

如要实现Hadoop的HA，必须保证在NN或RM出现故障时 可以让集群继续使用(避免单点故障)。

- HDFS ---> NN
  正在提供服务的NN， 必须和备用的NN保持相同的元数据
  步骤:

  1) 在active的nn**格式化**后，将**空白的fsimage**文件拷贝到**所有的nn**的机器上

  2) active的nn在**启动**后，将**edits文件**中的内容发送给**Journalnode**进程

  3) **standby状态**的nn**主动**从Journalnode进程拷贝数据，保证元数据的**同步**

  > 1.Journalnode在设计时，采用paxos协议, Journalnode适合在奇数台机器上启动！
  >
  > 在hadoop中，要求至少需要3个Journalnode进程Avilable.如果开启了hdfs的ha,不能再启动2nn
  >
  > 2.在同一时刻，最多只能有一个NN作为主节点，对外提供服务！其余的NN，作为备用节点！(否则不仅消耗性能 还容易出错)
  >
  > 3.使用active状态来标记主节点，使用standby状态标记备用节点！

  搭建:

  1.配置

  1) fs.defaultFS=hdfs://hadoop101:9000 进行修改

  2) 在整个集群中需要启动N个NN，配置N个NN运行的主机和开放的端口！

  3) 配置Journalnode

  2.启动

  1) 先启动Journalnode

  2) 格式化NN，精格式化后的fsimage文件同步到其他的NN

  3) 启动所有的NN，需要将其中之一转为active状态

- YARN ---> RM



压缩

1.压缩的目的
		压缩的目的是在MR运行期间，提高MR运行的效率！
		压缩可以减少MR运行期间的磁盘IO和网络IO！
		
2.压缩的原则
		IO密集型，多用压缩！
		计算密集型，CPU负载过重，少用压缩！



调度器

- FIFO 调度器

  单队列，所有的Job按照客户端提交的先后顺序，先到先服务

  >弊端：如果当前队列中有一个大的Job，非常消耗资源，那么这个Job之后的其他Job都需要付额外的等待时间！造成集群的资源利用率不足！
  >解决：   采取多队列的配置

- 容量调度器(Hadoop的默认调度器)

  本质是**多个FIFO**的队列组成

  >特点：容量
  >①每个队列可以**配置**一定的**容量**，空闲的资源可以**匀给其他**队列临时使用
  >②可以配置每个job使用的容量的限制，防止一个大的job独占所有资源
  >③可以配置每个用户可以使用的容量限制，防止当个用户占用所有资源
  >			
  >优点：  
  >
  >①配置灵活，及时刷新即可、
  >②资源利用率高
  >③安全，可以配置每个队列的访问用户限制

- 公平调度器

  设置和容量调度器大致相同，也是多条队列，每天队列都可以设置一定的容量。每个Job，用户可以设置容量

  >区别:
  >
  >公平调度器在调度策略上，采用**最大最小公平算法**，来调度Job，这个算法会保证同一个队列中，所有已经提交，未运行结束的Job，获取到队列中的资源是平等的！
  >导致在一个队列中，小的Job运行有优势，大的Job可能不能及时获取到必须的所有资源，但是不至于饿死！
  >
  >例如:
  >
  >当前队列A ： 目前有20个CPU，20G 内存...资源
  >
  >每个Job理论上应该分配  5个CPU， 5G内存，在实际分配资源时，**只考虑内存**
  >
  >队列A中已经提交，未运行的Job：
  >
  >job1 :  
  >
  >2 个MapTask   2 CPU,2G 内存
  >2 个ReduceTask   1 CPU,1G 内存
  >		
  >job2 ： 
  >
  >2 个MapTask   4 CPU,2G 内存
  >2 个ReduceTask   2 CPU,2G 内存
  >		
  >job3  :  
  >
  >1 个MapTask   1 CPU,1G 内存
  >1 个ReduceTask   1 CPU,1G 内存 
  >		
  >job4 : 
  >
  >4 个MapTask   4 CPU,2G 内存
  >4 个ReduceTask   2 CPU,2G 内存



推测执行

- 在MR运行期间，MRAppMaster会为最慢的任务启动一个备份任务，备份任务和当前任务先运行完的会采用其结果！
- 推测执行是典型的以空间换时间，因此在集群资源不足时，如果开启了推测执行，反而不能取得效果！
- 不适用场景：
  ①数据有倾斜
  ②特殊场景： 向数据库写记录



Hadoop的优化

- 小文件的优化
  		①源头上处理，在上传到集群之前，提前处理小文件
  		②小文件已经在HDFS存在，可以使用hadoop archieve进行归档
  		③在运行MR时，可以使用CombineTextInputFormat将多个小文件规划到一个切片中
  		④小文件过多，可以开启JVM重用

- MR的优化
  		核心：  

  - ①合理设置MapTask和ReduceTask的数量 

    ​	在合理利用资源的情况下，提高程序并行运行的效率

  - ②避免数据倾斜
    	Map端的数据倾斜：  在切片时，注意每片数据尽量均匀。防止有些不可切片的数据！
    	Reduce端的数据倾斜： 提前对数据进行**抽样调查**，统计出大致的分布范围，
    	根据分布范围，合理编写Partitioner，让每个分区的数据尽量均衡。

  - ③优化磁盘IO和网络IO
    					a)启用combiner
    					b)启动压缩
    					c)调大MapTask缓冲区的大小，减少溢写次数
    					d)调大MapTask中merge阶段一次合并的片段数，减少合并花费的时间
    					e)调大reduceTask中shuffle线程可以使用的内存，减少溢写次数
    					d)调大reduceTask中，input.buffer的大小，提前缓存部分数据到buffer中

